<!DOCTYPE html>
<html>
<head>
  <title>Practical Machine Learning Project</title>
  <meta charset="utf-8">
  <meta name="description" content="Practical Machine Learning Project">
  <meta name="author" content="Swathi S.">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  
  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">
    <h1>Practical Machine Learning Project</h1>
    <h2>Data Science Specialization - Coursera</h2>
    <p>Swathi S.<br/></p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Project Background</h2>
  </hgroup>
  <article data-timings="">
    <p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. The goal of this project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> (see the section on the Weight Lifting Exercise Dataset). </p>

<p>The training data for this project are available here:
<a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a></p>

<p>The test data are available here: 
<a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a></p>

<p>The data for this project come from this source: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Loading and cleaning up the data</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Data is loaded.</li>
<li>Columns filled with NA values are deleted as they are not going to be good predictors</li>
<li>The first 5 columns are also removed as they contain only descriptive information like name, timestamps.</li>
<li>Training data is further split into </li>
</ul>

<pre><code class="r">options(warn=-1)
suppressMessages(library(caret))
Train &lt;- read.csv(&quot;pml-training.csv&quot;)
Train &lt;- Train[colSums(is.na(Train))==0] #Columns with NA values deleted
Train &lt;- Train[,6:93]                    #Descriptive columns deleted
inTrain &lt;- createDataPartition(y=Train$classe,p=0.75,list=FALSE)
training &lt;- Train[inTrain,]
testing &lt;- Train[-inTrain,]
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Predictor Selection</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Near zero covariates are removed</li>
</ul>

<pre><code class="r">nsv &lt;- nearZeroVar(training, saveMetrics = T)
head(nsv,7)
</code></pre>

<pre><code>##                      freqRatio percentUnique zeroVar   nzv
## new_window           44.566563     0.0135888   FALSE  TRUE
## num_window            1.133333     5.8228020   FALSE FALSE
## roll_belt             1.120536     7.8067672   FALSE FALSE
## pitch_belt            1.013245    11.7203424   FALSE FALSE
## yaw_belt              1.014851    12.3929882   FALSE FALSE
## total_accel_belt      1.037206     0.1902432   FALSE FALSE
## kurtosis_roll_belt 1799.375000     2.1470308   FALSE  TRUE
</code></pre>

<pre><code class="r">training &lt;- training[, !nsv$nzv]
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Training the model using Decision Tree</h2>
  </hgroup>
  <article data-timings="">
    <p>The first method used to train the data is using Prediction Trees</p>

<pre><code class="r">suppressMessages(library(rpart))
suppressMessages(library(rpart.plot))
tree &lt;- rpart(classe~.,method=&quot;class&quot;,data=training)
prp(tree)
</code></pre>

<p><img src="figure/unnamed-chunk-3-1.png" alt="plot of chunk unnamed-chunk-3"> </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Evaluating Decision Tree</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">predTree &lt;- predict(tree,newdata=testing,type=&quot;class&quot;)
confusionMatrix(predTree,testing$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1261  146   12   39   19
##          B  106  696   96  201  172
##          C    0   57  676   38    2
##          D   25   41   68  460   91
##          E    3    9    3   66  617
## 
## Overall Statistics
##                                           
##                Accuracy : 0.7565          
##                  95% CI : (0.7443, 0.7685)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.6909          
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9039   0.7334   0.7906   0.5721   0.6848
## Specificity            0.9384   0.8546   0.9760   0.9451   0.9798
## Pos Pred Value         0.8538   0.5476   0.8745   0.6715   0.8840
## Neg Pred Value         0.9609   0.9304   0.9567   0.9185   0.9325
## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
## Detection Rate         0.2571   0.1419   0.1378   0.0938   0.1258
## Detection Prevalence   0.3012   0.2592   0.1576   0.1397   0.1423
## Balanced Accuracy      0.9212   0.7940   0.8833   0.7586   0.8323
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Training the model using Decision Tree with Cross Validation</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Define cross-validation experiment</li>
<li>Perform the cross validation</li>
<li>Create a new CART model</li>
</ul>

<pre><code class="r">suppressMessages(library(e1071))
set.seed(1)
fitControl &lt;- trainControl(method=&quot;cv&quot;,number=10)
cartGrid &lt;- expand.grid(.cp=(1:50)*.01)
cv &lt;- train(classe~.,data=training,method=&quot;rpart&quot;,trControl=fitControl,tuneGrid=cartGrid) #From this we find that best control parameter for highest accuracy is cp=0.01
treeCV &lt;- rpart(classe~.,data=training,method=&quot;class&quot;,control=rpart.control(cp=0.01))
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Evaluating Decision Tree with Cross Validation</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">predTreeCV &lt;- predict(treeCV,newdata=testing,type=&quot;class&quot;)
confusionMatrix(predTreeCV,testing$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1261  146   12   39   19
##          B  106  696   96  201  172
##          C    0   57  676   38    2
##          D   25   41   68  460   91
##          E    3    9    3   66  617
## 
## Overall Statistics
##                                           
##                Accuracy : 0.7565          
##                  95% CI : (0.7443, 0.7685)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.6909          
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9039   0.7334   0.7906   0.5721   0.6848
## Specificity            0.9384   0.8546   0.9760   0.9451   0.9798
## Pos Pred Value         0.8538   0.5476   0.8745   0.6715   0.8840
## Neg Pred Value         0.9609   0.9304   0.9567   0.9185   0.9325
## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
## Detection Rate         0.2571   0.1419   0.1378   0.0938   0.1258
## Detection Prevalence   0.3012   0.2592   0.1576   0.1397   0.1423
## Balanced Accuracy      0.9212   0.7940   0.8833   0.7586   0.8323
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Training the model with Random Forests</h2>
  </hgroup>
  <article data-timings="">
    <p>In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally during the run. However, the error does decrease with the number of trees. </p>

<pre><code class="r">suppressMessages(library(randomForest))
set.seed(1)
forest &lt;- randomForest(classe~.,data=training,ntree=200,nodesize=25)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Evaluating Random Forests</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">predForest &lt;- predict(forest, newdata=testing)
confusionMatrix(predForest,testing$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1394    4    0    0    0
##          B    1  944   15    0    0
##          C    0    1  840    8    0
##          D    0    0    0  794    3
##          E    0    0    0    2  898
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9931          
##                  95% CI : (0.9903, 0.9952)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9912          
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9993   0.9947   0.9825   0.9876   0.9967
## Specificity            0.9989   0.9960   0.9978   0.9993   0.9995
## Pos Pred Value         0.9971   0.9833   0.9894   0.9962   0.9978
## Neg Pred Value         0.9997   0.9987   0.9963   0.9976   0.9993
## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
## Detection Rate         0.2843   0.1925   0.1713   0.1619   0.1831
## Detection Prevalence   0.2851   0.1958   0.1731   0.1625   0.1835
## Balanced Accuracy      0.9991   0.9953   0.9901   0.9934   0.9981
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Conclusion</h2>
  </hgroup>
  <article data-timings="">
    <p>Out of the three models that were tried, it is clear that Random Forest is the best model with an accuracy of 99.3%. With Decision Tree, both models - with and without cross validation gave an accuracy of above 70%. It is interesting to note that there was no noticable improvement in performance of the decision tree even after performing a 10 fold cross validation.</p>

<p>Let&#39;s apply the Random Forest model to the test data set that was provided. From our model we can expect an out of sample error of 0.7% </p>

<pre><code class="r">finalTest &lt;- read.csv(&quot;pml-testing.csv&quot;)
Final_Prediction&lt;-predict(forest,finalTest)
Final_Prediction
</code></pre>

<pre><code>##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
## Levels: A B C D E
</code></pre>

<p>The Random Forest model has correctly predicted all values as verified by online submission.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='Project Background'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='Loading and cleaning up the data'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='Predictor Selection'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Training the model using Decision Tree'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='Evaluating Decision Tree'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='Training the model using Decision Tree with Cross Validation'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='Evaluating Decision Tree with Cross Validation'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='Training the model with Random Forests'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Evaluating Random Forests'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='Conclusion'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title=''>
         11
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>